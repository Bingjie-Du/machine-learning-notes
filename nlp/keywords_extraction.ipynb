{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键词抽取\n",
    "\n",
    "\n",
    "* TF-IDF\n",
    "* TextRank\n",
    "* [EmbedRank](https://github.com/luozhouyang/embedrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "* **TF**: term frequency, 词语在文档中出现的次数\n",
    "* **IDF**: inverse doucment frequence, 包含改词语的文档占总文档数量的比例的倒数\n",
    "\n",
    "$$tf = \\frac{count(w)}{\\sum_{w_i} count(w_i)}$$\n",
    "\n",
    "$$idf = \\log{\\frac{N}{\\sum_{i=1}^N I(w, N_i)}}$$\n",
    "\n",
    "防止分母为零，需要平滑处理，一般采用 **+1** 平滑\n",
    "\n",
    "$$idf = \\log{\\frac{N+1}{\\sum_{i=1}^N I(w, N_i) + 1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordsExtractor:\n",
    "    \n",
    "    def __init__(self, stopwords_file=None):\n",
    "        self.stopwords = self._load_stopwords(stopwords_file) if stopwords_file else None\n",
    "        \n",
    "    def _load_stopwords(self, file):\n",
    "        words = set()\n",
    "        if not os.path.exists(file):\n",
    "            print('File %s does not exist.' % file)\n",
    "            return words\n",
    "        with open(file, mode='rt', encoding='utf8') as fin:\n",
    "            for lin in fin:\n",
    "                line = line.strip('\\n').strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                words.add(line)\n",
    "        return words\n",
    "    \n",
    "    def extract_keywords(self, document, *args, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFKeywordsExtractor(KeywordsExtractor):\n",
    "    \n",
    "    def __init__(self, idf_file, stopwords_file=None):\n",
    "        super().__init__(stopwords_file=stopwords_file)\n",
    "        self.idfmap = self._load_idf(idf_file) if idf_file else dict()\n",
    "        self.median_idf = sorted(self.idfmap.values())[len(self.idfmap)//2]\n",
    "        \n",
    "    def _load_idf(self, file):\n",
    "        m = dict()\n",
    "        if not os.path.exists(file):\n",
    "            print('File %s does not exist.' % file)\n",
    "            return m\n",
    "        with open(file, mode='rt', encoding='utf8') as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip('\\n').strip()\n",
    "                parts = line.split(' ')\n",
    "                if len(parts) != 2:\n",
    "                    continue\n",
    "                m[parts[0]] = float(parts[1])\n",
    "        return m\n",
    "    \n",
    "    def extract_keywords(self, document, topk=20):\n",
    "        freq = {}\n",
    "        for word in jieba.cut(document):\n",
    "            word = word.strip()\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            if word in self.stopwords:\n",
    "                continue\n",
    "            freq[word] = freq.get(word, 0) + 1\n",
    "        \n",
    "        total_freq = sum(freq.values())\n",
    "        idf = {}\n",
    "        for k in freq.keys():\n",
    "            idf[k] = freq[k] * self.idfmap.get(k, self.median_idf)\n",
    "        return sorted(idf.items(), key=lambda x:x[1], reverse=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idf一般需要大量的数据统计得到。\n",
    "\n",
    "pyspark提供了教程：\n",
    "\n",
    "* [ml-features](https://spark.apache.org/docs/latest/ml-features)\n",
    "* [tf-idf](https://spark.apache.org/docs/latest/ml-features#tf-idf)\n",
    "\n",
    "以下是一个使用spark在Hadoop统计idf的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -i https://mirrors.aliyun.com/pypi/simple pyspark numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "import jieba\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import HiveContext, SparkSession\n",
    "from pyspark import Row\n",
    "from pyspark.ml.feature import IDF, HashingTF, Tokenizer\n",
    "\n",
    "\n",
    "jieba.initialize()\n",
    "\n",
    "\n",
    "def get_spark(master='local[*]', app_name='idf'):\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(master) \\\n",
    "        .config('spark.executor.memory', '8g') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.cores.max', '8') \\\n",
    "        .config('spark.driver.memory', '8g') \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "def _collect_documents(x):\n",
    "    segs = x.split('\\t')\n",
    "    if len(segs) != 9:\n",
    "        return []\n",
    "    jd_json = segs[8]\n",
    "    jd_json = re.sub(r'\\n\\t', '', jd_json)\n",
    "    jd_json = re.sub(r'\\\\s+', ' ', jd_json)\n",
    "    jd_json = jd_json.lower()\n",
    "    return [jd_json]  # 整个JD作为一个document\n",
    "\n",
    "\n",
    "def _tokenize(x):\n",
    "    words = []\n",
    "    for w in jieba.cut(x):\n",
    "        w = w.strip()\n",
    "        if not w:\n",
    "            continue\n",
    "        words.append(w)\n",
    "    return words\n",
    "\n",
    "\n",
    "def _idf_flat_map(x):\n",
    "    items = []\n",
    "    for w, _id, tf, idf in zip(x.words, x.tf.indices, x.tf.values, x.idf.values):\n",
    "        items.append((w, idf))\n",
    "    return items\n",
    "\n",
    "\n",
    "def _debug(x):\n",
    "    print(type(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "def _filter_idf(x):\n",
    "    w, v = x[0], x[1]\n",
    "    if len(w) <= 1:\n",
    "        return False\n",
    "    if re.match(r'^[0-9]+$', x):\n",
    "        return False\n",
    "    if re.match(r'[0-9]{6,}', x):\n",
    "        return False\n",
    "    if re.match(r'^[0-9]+.[0-9]+$', w):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def calculate(input_path, output_path, parts=16):\n",
    "    spark = get_spark()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    rdd = sc.textFile(input_path)\n",
    "    rdd = rdd.filter(lambda x: len(x.split('\\t')) == 9)\n",
    "    rdd = rdd.flatMap(_collect_documents).filter(lambda x: x)\n",
    "    rdd = rdd.map(_tokenize).filter(lambda x: x).map(lambda x: Row(words=x))\n",
    "    # rdd = rdd.map(_debug)\n",
    "\n",
    "    df = rdd.toDF()\n",
    "    # numFeatures即hash桶数\n",
    "    hashingTF = HashingTF(inputCol='words', outputCol='tf', numFeatures=2 << 20)\n",
    "    featuredData = hashingTF.transform(df)\n",
    "\n",
    "    idf = IDF(inputCol='tf', outputCol='idf')\n",
    "    idfModel = idf.fit(featuredData)\n",
    "    res = idfModel.transform(featuredData)\n",
    "\n",
    "    rdd = res.rdd.flatMap(_idf_flat_map).reduceByKey(lambda a, b: a).sortBy(lambda x: x[0], ascending=True)\n",
    "    rdd = rdd.filter(_filter_idf)\n",
    "    rdd = rdd.map(lambda x: x[0] + '\\t' + str(x[1]))\n",
    "    rdd.repartition(parts).saveAsTextFile(output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'hdfs:///basic_data/tob/tob_ats/recruit_step_v3/part-00099-8d87777f-34ee-431a-be5d-8a6f0b92fea9-c000.txt'\n",
    "    output_file = 'hdfs:///user/kdd_luozhouyang/idf/jd/20200509'\n",
    "    calculate(input_file, output_file, parts=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -i https://mirrors.aliyun.com/pypi/simple networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import networkx as nx\n",
    "import jieba.posseg as jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRankKeywordsExtractor(KeywordsExtractor):\n",
    "    \n",
    "    def _unique_tokens(self, all_words):\n",
    "        words = []\n",
    "        for k in all_words:\n",
    "            if k in words:\n",
    "                continue\n",
    "            words.append(k)\n",
    "        return words\n",
    "    \n",
    "    def _edit_distance(self, a, b):\n",
    "        m, n = len(a)-1, len(b)-1\n",
    "        dp = [[0]*(n+1) for _ in range(m+1)] # (m+1)*(n+1)\n",
    "        for i in range(m+1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(n+1):\n",
    "            dp[0][j] = j\n",
    "        for i in range(1, m+1):\n",
    "            for j in range(1, n+1):\n",
    "                if a[i-1] == b[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1]\n",
    "                else:\n",
    "                    dp[i][j] = 1 + max(dp[i-1][j], dp[i][j-1])\n",
    "        return dp[m][n]\n",
    "    \n",
    "    def _build_graph(self, words):\n",
    "        g = nx.Graph()\n",
    "        g.add_nodes_from(words)\n",
    "        pairs = list(itertools.combinations(words, 2))\n",
    "        \n",
    "        for p in pairs:\n",
    "            first, second = p[0], p[1]\n",
    "            # 使用编辑距离来作为词语的相似度衡量，可以使用其他方式\n",
    "            ed = self._edit_distance(first, second)\n",
    "            g.add_edge(first, second, weight=ed)\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def extract_keywords(self, document):\n",
    "        words = [w.strip() for w in jieba.cut(document) if w.strip()]\n",
    "        unique_words = self._unique_tokens(words)\n",
    "        \n",
    "        graph = self._build_graph(unique_words)\n",
    "        textrank = nx.pagerank(graph, weight='weight')\n",
    "        print(textrank)\n",
    "        # 所有的节点\n",
    "        keyphrase = sorted(textrank, key=textrank.get, reverse=True)\n",
    "        # 取1/3\n",
    "        keyphrase = keyphrase[0:len(unique_words)//3 + 1]\n",
    "        print(keyphrase)\n",
    "        \n",
    "        # 相邻的词合并成短语\n",
    "        res, dealt = set(), set()\n",
    "        i, j = 0, 0\n",
    "        while j < len(words):\n",
    "            a, b = words[i], words[j]\n",
    "            if a in keyphrase and b in keyphrase:\n",
    "                res.add(a + ' ' + b)\n",
    "                dealt.add(a)\n",
    "                dealt.add(b)\n",
    "            else:\n",
    "                if a in keyphrase and a not in dealt:\n",
    "                    res.add(a)\n",
    "                if j == len(words)-1 and b in keyphrase and b not in dealt:\n",
    "                    res.add(b)\n",
    "            i += 1\n",
    "            j += 1\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank = TextRankKeywordsExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'java': 0.37078347266331135, '开发': 0.2962171231279811, '工程师': 0.3329994042087073}\n",
      "['java', '工程师']\n",
      "{'java java', '工程师 工程师'}\n"
     ]
    }
   ],
   "source": [
    "res = textrank.extract_keywords('java开发工程师')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine-learning-notes] *",
   "language": "python",
   "name": "conda-env-machine-learning-notes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
